services:
  # DB Service
  db:
    image: postgres:16
    container_name: exceed-postgres
    restart: always
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin
      POSTGRES_DB: prolific
    volumes:
      - db_data:/var/lib/postgresql/data
    networks:
      - exceed-prolific-network

  # LLM Service
  ollama:
    image: ollama/ollama:latest
    container_name: exceed-ollama
    restart: always
    volumes:
      - ollama_data:/var/lib/ollama
    # Make sure to pull the model after the service is up and running
    command: >
      sh -c "ollama serve && until curl -s http://localhost:11434/health; do sleep 1; done && ollama pull llama3.1:8b && tail -f /dev/null"
    healthcheck:
      test: [ "CMD", "curl", "-sf", "http://localhost:11434/api/tags | grep -q 'llama3.1:8b'" ]
      interval: 30s
      timeout: 5s
      retries: 10

  # FastAPI Backend Service
  backend:
    build:
      context:
        ./exceed-prolific-backend
    container_name: exceed-backend
    env_file:
      - ./exceed-prolific-backend/.env
    depends_on:
      ollama:
        condition: service_healthy
      db:
        condition: service_healthy

  # NextJS Frontend Service
  frontend:
    build:
      context:
        ./exceed-prolific-frontend
    container_name: exceed-frontend
    restart: always
    environment:
      - NEXT_PUBLIC_BACKEND_HOST=http://localhost:8000
    depends_on:
      - backend

  # Nginx reverse proxy
  nginx:
    image: nginx:alpine
    container_name: nginx-proxy
    restart: always
    ports:
      - "80:80"
    volumes:
      - ./nginx/default.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - frontend
      - backend
    networks:
      - app-network

networks:
  exceed-prolific-network:
    driver: bridge

volumes:
  db_data:
    driver: local
  ollama_data:
    driver: local
